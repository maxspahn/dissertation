\section{Interaction and Teaching Capabilities}
\label{sec:teaching}

To quickly adapt our robot to new store environments and
products, we created four interfaces for operators: a
digital twin for remote monitoring and control, adding
product classes to the perception, trajectory teaching mode,
and audio explanations of the robot's symbolic actions.

\input{src/24-spahn-rss/src/remote}
\begin{figure}[t]
  \centering
  \begin{subfigure}[b]{0.45\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{teaching_005_rounded.png}
    \caption{}
    \label{subfig:teaching_1}
  \end{subfigure}
  \begin{subfigure}[b]{0.45\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{teaching_009_rounded.png}
    \caption{}
    \label{subfig:teaching_2}
  \end{subfigure}
  \begin{subfigure}[b]{0.45\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{teaching_011_rounded.png}
    \caption{}
    \label{subfig:teaching_3}
  \end{subfigure}
  \begin{subfigure}[b]{0.45\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{teaching_014_rounded.png}
    \caption{}
    \label{subfig:teaching_4}
  \end{subfigure}
  \caption{The human operator can easily `teach' the robot a new picking
  strategy by moving the arm, thus passing implicit knowledge to the
  robot.}
  \label{fig:recording}
\end{figure}
\subsection{Interactively adding product classes to perception}
\cref{sec:perception} explains ProtoProductNet, our
adaptation to the state-of-the-art ProtoNet, to make the
few-shot learning approach scalable for the supermarket
environment. To add new unseen product classes to
ProtoProductNet, we developed a custom user interaction for
human operators. The interaction contains the following
steps 1) the operator uses a barcode scanner, attached to
the robot, to scan the new product 2) the operator puts the
product in view in front of the robot's camera 3) a GUI with the view of the camera pops up on the screen and the operator interactively drags a box around the new product. The robot should already start detecting the product, as can be verified by a bounding box appearing on screen. To further enhance the detection the operator can add images from different angles of the product. The cropped images selected by the operator are saved locally and combined with the product's barcode. If the same barcode is encountered in future orders, our perception pipeline will now know how to classify the product accurately, without retraining the network.

\subsection{Teaching grasping trajectories to the robot}
\label{sub:teaching}

As outlined in \cref{sec:trajectory_generation},
\ac{fabrics} require global guidance to effectively execute
complex symbolic actions that are essential for some products, see
\cref{fig:product_examples}.
To simplify the process of obtaining this guidance in the form of trajectories, we leverage human expert demonstrations, effectively teaching the robot.
We distinguish between two
phases for teaching, the \textit{recording} and the
\textit{playback}. For both phases, we assume that the product
is visible and detected by the robot, such that we can
compute a transformation between the root link and the product.

\subsubsection{Recording}
When recording a trajectory, we first reduce the stiffness
of the robot to the bare minimum, such that it can easily be
pushed around by the human operator. Then, the human
operator can activate recording by pressing a button on our tablet interface. From that moment onwards, the state values \x{} for the
constraints defined for picking in \cref{sub:picking}
are recorded, see \cref{fig:recording}.
The state of the gripper, active
or non-active, and whether a product is attached to the gripper
are also recorded.
The generated sequence of constraint values and gripper
states is stored as a reference trajectory.

%For the sake of memory efficiency, we
%ignore trajectory points if they are too similar to the
%previous one.
We additionally record the
transformation matrix between the root link of our kinematic
chain and the product to be picked. That allows us to later
generalize the recording to different product poses by applying a rigid body transformation to the trajectory.


\subsubsection{Playback}

During trajectory playback, we loop through the recorded goals sequentially, continuing when a desired goal accuracy has been reached. 
%When playing back a trajectory, the trajectory points are
%passed to the trajectory generator as goals in a sequential
%manner. Proceeding to the subsequent trajectory point
%requires that the current one has been reached up to a
%desired accuracy. 
\begin{figure}[ht]
  \centering
  \includegraphics[width=1\linewidth]{trajectory_transformation.pdf}
  \caption{To generalize to different item poses, recorded
  trajectories (red) are transformed based on the the
  transformation between the item's pose during recording
  and during playback (orange). The new trajectory (blue) is then
  tracked using our trajectory generation method.
  }
  \label{fig:transformation_trajectory}
\end{figure}
%
In contrast to the recording part, where we exclude motion
of the base, during playback the base motion is activated,
see \cref{fig:base_motion}. To account for different product
poses between recording and playback, we transform the goals
on the fly based on the product pose estimate, see
\cref{sec:perception}, using the following transform:
\[
  _{\product,r}\mat{T}^{\product,p} =
  \left(_{\textrm{base}}\mat{T}^{\product,r}\right)^{-1}_{\textrm{base}}\mat{T}^{\product,p},
\]
where $_{\textrm{base}}\mat{T}^{\product,r}$ is the transformation matrix
between the manipulator's base link and the product during
recording and $_{\textrm{base}}\mat{T}^{\product,p}$ is the
transformation matrix between the manipulator's base link and
the product during playback, see
\cref{fig:transformation_trajectory}.
Using this continual feedback we effectively employ a visual servoing \cite{kmich2022image} approach and are robust against changes in product location during the pick.
In addition to \ac{fabrics} goals, the recording also contains information about the state of the vacuum pump. This state information is replicated during playback, and used to know when a product should have been attached.
In the playback routine for picking we then modify the fabrics goal if a product is not yet attached where it is expected. The goal is modified to effectively push further into the shelf along the z-axis of the nozzle head, until a product is attached, or a maximum threshold is reached.
\begin{figure*}[t]
  \centering
  \begin{subfigure}[b]{0.20\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{base_motion_002_rounded_wo.png}
    \caption{}
    \label{subfig:base_motion_1}
  \end{subfigure}
  \begin{subfigure}[b]{0.20\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{base_motion_003_rounded_wo.png}
    \caption{}
    \label{subfig:base_motion_2}
  \end{subfigure}
  \begin{subfigure}[b]{0.20\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{base_motion_004_rounded_wo.png}
    \caption{}
    \label{subfig:base_motion_3}
  \end{subfigure}
  \begin{subfigure}[b]{0.20\linewidth}
    \centering
    \includegraphics[width=0.95\textwidth]{base_motion_005_rounded_wo.png}
    \caption{}
    \label{subfig:base_motion_4}
  \end{subfigure}
  \caption{During playback, \ac{fabrics}  actively use the base's forward motion as a prismatic joint to compensate for misplacement during navigation.}
  \label{fig:base_motion}
\end{figure*}
%
\subsection{Generalizability}
\label{sub:generalizability}

To reduce the number of taught trajectories, we rely
on generic trajectories. Our gripper design led to 
a \textit{horizontal} and a \textit{vertical} pick
trajectory, where the two suction cups are either aligned
horizontally or vertically. As most considered products have a planar surface, we can use these two
trajectories for most products. These trajectories are
replaced by product specific trajectories in case of
repeated failures. For example, unconventional bottle
shapes require modified trajectories. Similar to existing
works on learning-from-demonstration \cite{argall2009survey}, we
argue that non-experts can, over time, create an increasingly
complete trajectory database for all products to further
improve performance. Importantly, general trajectories have
proven to be sufficient for most of our products.
Note that all trajectories are robot agnostic and only
gripper specific, so we expect them to be transferable to
other robots with similar gripper designs.
The generalizability of our approach relies on the
transformation of trajectories according to the item's pose. The robot's workspace is a natural limitation, as items placed outside the workspace
(i.e. on the lowest or highest shelf or at the back on the
shelf) are kinematically unreachable and thus, not resolvable by our teaching approach.


\subsection{Audio feedback}
\label{sec:language_feedback}

During the robot's operation, we are interested in providing
audio explanations of the robot's actions as feedback to
operators, e.g., to monitor the robot and to be notified
about failures. We do this by 1) generating compact prompts
of the robot's action and state and 2) using Large Language
Models (LLMs) to generate short informative explanations
that are played via the robot's speaker.


\paragraph{Prompt generation}
We leverage the structure of the generated \acp{bt} and
symbolic state information (see \cref{sec:decision_making})
to generate prompts for an LLM. For every sub-tree in the
generated \ac{bt}, we automatically add explanation
nodes that
generate prompts for symbolic actions and items. An
explanation is
described by its name $a$ formulated as a verb in the
present continuous form, e.g., $a=\texttt{placing}$. The
item's name $i$ is taken from the product database, e.g.,
$i=\textit{Whole Milk}$. We generate string prompts of the
form $pr$:
\begin{equation*}
    pr := \textit{action }~a~i~c,
\end{equation*}
where $c\in\{\texttt{running},\texttt{failed}, \texttt{completed}, \texttt{retry}\}$ is the status returned from the sub-tree.
An example for a generated prompt is 
\begin{center}
``\textit{action }\texttt{picking }\texttt{Whole Milk }\texttt{retry}''.
\end{center}


\paragraph{Explanation generation}
We generate explanations by prompting an LLM on the fly with our generated prompts. 
To this end, we provide the LLM with a context describing
that the robot is deployed as an order picking robot in a
supermarket with five symbolic actions and that the task is to generate a concise explanation of the prompt to operators. 
During operation of the robot, we simultaneously generate the explanations and play them back via the robot's speaker.
For instance, for the prompt  ``\textit{action }\texttt{picking }\texttt{Whole Milk }\texttt{retry}'', we generate the explanation: ``Oops! It seems like I had a little trouble placing the Whole Milk into my basket. No worries, I'll give it another
  try and make sure it goes in smoothly this time''.

