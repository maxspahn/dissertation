\begin{abstract}
In the previous chapters, advancements to the framework of 
\ac{fabrics} have been proposed to allow for the deployment into dynamic
environments and the automation of tuning. However, all of these methods, relied
on simplistic environment representations, usually composed of primitive shapes.
In this chapter, we integrate implicit environment representations known from
mobile robotics and drones into the framework of \ac{fabrics}. We show that low
computational costs and the existence of a closed-form solution can be exploited
to relax the requirements on the perception pipeline. We quantitatively compare
different level of expressiveness, starting from shape-based collision avoidance
and finishing with raw point clouds.
\end{abstract}
